{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Ethan Paek\n",
    "\n",
    "Date: 4/22/2020\n",
    "\n",
    "Topic: COEN 140 Lab 4\n",
    "\n",
    "Description: Use Linear Discriminant Analysis (LDA) and Quadratic Discriminant Analysis (QDA) over the provided dataset.The dataset can be downloaded at http://www.cse.scu.edu/~yfang/coen140/iris.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import pprint\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import data and breakup into training and testing subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0    1    2    3               4\n",
      "0    5.1  3.5  1.4  0.2     Iris-setosa\n",
      "1    4.9  3.0  1.4  0.2     Iris-setosa\n",
      "2    4.7  3.2  1.3  0.2     Iris-setosa\n",
      "3    4.6  3.1  1.5  0.2     Iris-setosa\n",
      "4    5.0  3.6  1.4  0.2     Iris-setosa\n",
      "..   ...  ...  ...  ...             ...\n",
      "145  6.7  3.0  5.2  2.3  Iris-virginica\n",
      "146  6.3  2.5  5.0  1.9  Iris-virginica\n",
      "147  6.5  3.0  5.2  2.0  Iris-virginica\n",
      "148  6.2  3.4  5.4  2.3  Iris-virginica\n",
      "149  5.9  3.0  5.1  1.8  Iris-virginica\n",
      "\n",
      "[150 rows x 5 columns]\n",
      "<class 'numpy.float64'>\n"
     ]
    }
   ],
   "source": [
    "# load and store dataset from website\n",
    "data = pd.io.parsers.read_csv(\n",
    "    filepath_or_buffer='http://www.cse.scu.edu/~yfang/coen140/iris.data',\n",
    "    header=None,\n",
    "    sep=',',\n",
    "    )\n",
    "print(data)\n",
    "print(type(data[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 5)\n",
      "(30, 5)\n"
     ]
    }
   ],
   "source": [
    "# split up 80% of dataset for training and 20% for testing\n",
    "train = data[0:40].append(data[50:90]).append(data[100:140])\n",
    "test  = data[40:50].append(data[90:100]).append(data[140:150])\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0    1    2    3            4\n",
      "35  5.0  3.2  1.2  0.2  Iris-setosa\n",
      "36  5.5  3.5  1.3  0.2  Iris-setosa\n",
      "37  4.9  3.1  1.5  0.1  Iris-setosa\n",
      "38  4.4  3.0  1.3  0.2  Iris-setosa\n",
      "39  5.1  3.4  1.5  0.2  Iris-setosa \n",
      "\n",
      "      0    1    2    3                4\n",
      "85  6.0  3.4  4.5  1.6  Iris-versicolor\n",
      "86  6.7  3.1  4.7  1.5  Iris-versicolor\n",
      "87  6.3  2.3  4.4  1.3  Iris-versicolor\n",
      "88  5.6  3.0  4.1  1.3  Iris-versicolor\n",
      "89  5.5  2.5  4.0  1.3  Iris-versicolor \n",
      "\n",
      "       0    1    2    3               4\n",
      "135  7.7  3.0  6.1  2.3  Iris-virginica\n",
      "136  6.3  3.4  5.6  2.4  Iris-virginica\n",
      "137  6.4  3.1  5.5  1.8  Iris-virginica\n",
      "138  6.0  3.0  4.8  1.8  Iris-virginica\n",
      "139  6.9  3.1  5.4  2.1  Iris-virginica\n"
     ]
    }
   ],
   "source": [
    "# categorize our flowers by their respective class and create separate tables for them\n",
    "setosa = train[train[4] == 'Iris-setosa']\n",
    "print(setosa.tail(),'\\n')\n",
    "versicolor = train[train[4] == 'Iris-versicolor']\n",
    "print(versicolor.tail(),'\\n')\n",
    "virginica = train[train[4] == 'Iris-virginica']\n",
    "print(virginica.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setosa:\n",
      "     0    1    2    3\n",
      "0  5.1  3.5  1.4  0.2\n",
      "1  4.9  3.0  1.4  0.2\n",
      "2  4.7  3.2  1.3  0.2\n",
      "3  4.6  3.1  1.5  0.2\n",
      "4  5.0  3.6  1.4  0.2 \n",
      "\n",
      "versicolor:\n",
      "      0    1    2    3\n",
      "50  7.0  3.2  4.7  1.4\n",
      "51  6.4  3.2  4.5  1.5\n",
      "52  6.9  3.1  4.9  1.5\n",
      "53  5.5  2.3  4.0  1.3\n",
      "54  6.5  2.8  4.6  1.5 \n",
      "\n",
      "virginica:\n",
      "       0    1    2    3\n",
      "100  6.3  3.3  6.0  2.5\n",
      "101  5.8  2.7  5.1  1.9\n",
      "102  7.1  3.0  5.9  2.1\n",
      "103  6.3  2.9  5.6  1.8\n",
      "104  6.5  3.0  5.8  2.2 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop labels so that we only have numerical data for training sets\n",
    "train_setosa = setosa.drop(4,axis=1)\n",
    "print('setosa:')\n",
    "print(train_setosa.head(),'\\n')\n",
    "train_versicolor = versicolor.drop(4,axis=1)\n",
    "print('versicolor:')\n",
    "print(train_versicolor.head(),'\\n')\n",
    "train_virginica = virginica.drop(4,axis=1)\n",
    "print('virginica:')\n",
    "print(train_virginica.head(),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the probability density function (as noted from Dr. Fang's lectures)\n",
    "def P(x,mean,covariance):\n",
    "    k = x.shape[0]\n",
    "    first = 1/math.sqrt(((2.0*math.pi)**k) * np.linalg.det(covariance)) \n",
    "    second = math.exp(-0.5*np.dot(np.dot((x-mean),np.linalg.inv(covariance)),(x-mean)[np.newaxis].T))\n",
    "    return first*second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in order to use the function from above, we have to calculate the mean for each of the flowers' attributes\n",
    "def my_means(matrix):\n",
    "    means = []\n",
    "    for attribute in matrix.values.T:\n",
    "        means.append(attribute.sum()/float(matrix.shape[0]))\n",
    "    return np.array(means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we also need to calculate the covariances\n",
    "def my_covs(matrix):\n",
    "    mean = my_means(matrix)\n",
    "    # setup each class to have a 4 x 4 matrix for covariance\n",
    "    total_cov = np.zeros((matrix.shape[1],matrix.shape[1]))\n",
    "    for row in range(matrix.shape[0]):\n",
    "        total_cov += np.outer((matrix.iloc[row].values - mean),(matrix.iloc[row].values - mean))\n",
    "    cov = total_cov/float(matrix.shape[0])\n",
    "    return cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Means:\n",
      "{'setosa': array([5.0375, 3.44  , 1.4625, 0.2325]),\n",
      " 'versicolor': array([6.01  , 2.78  , 4.3175, 1.35  ]),\n",
      " 'virginica': array([6.6225, 2.96  , 5.6075, 1.99  ])}\n",
      "\n",
      "Covariances:\n",
      "{'setosa': array([[0.12784375, 0.0965    , 0.01265625, 0.01328125],\n",
      "       [0.0965    , 0.1294    , 0.002     , 0.0142    ],\n",
      "       [0.01265625, 0.002     , 0.02884375, 0.00446875],\n",
      "       [0.01328125, 0.0142    , 0.00446875, 0.00969375]]),\n",
      " 'versicolor': array([[0.2669    , 0.08445   , 0.167825  , 0.051     ],\n",
      "       [0.08445   , 0.1081    , 0.07885   , 0.04425   ],\n",
      "       [0.167825  , 0.07885   , 0.19844375, 0.071875  ],\n",
      "       [0.051     , 0.04425   , 0.071875  , 0.042     ]]),\n",
      " 'virginica': array([[0.45624375, 0.10765   , 0.34883125, 0.049975  ],\n",
      "       [0.10765   , 0.1104    , 0.07905   , 0.0451    ],\n",
      "       [0.34883125, 0.07905   , 0.33669375, 0.057825  ],\n",
      "       [0.049975  , 0.0451    , 0.057825  , 0.0724    ]])}\n"
     ]
    }
   ],
   "source": [
    "means = {}\n",
    "means['setosa']  = my_means(train_setosa)\n",
    "means['versicolor'] = my_means(train_versicolor)\n",
    "means['virginica'] = my_means(train_virginica)\n",
    "\n",
    "covs = {}\n",
    "covs['setosa'] = my_covs(train_setosa)\n",
    "covs['versicolor'] = my_covs(train_versicolor)\n",
    "covs['virginica'] = my_covs(train_virginica)\n",
    "\n",
    "# this variable is necessary for LDA, since the covariances are assumed to be equal\n",
    "cov_avg = (covs['setosa'] + covs['versicolor'] + covs['virginica'])/3.0\n",
    "\n",
    "# pprint is a module that prints out dictionaries all pretty :D\n",
    "print(\"Means:\")\n",
    "pprint.pprint(means)\n",
    "print(\"\\nCovariances:\")\n",
    "pprint.pprint(covs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Build an LDA classifier based on the training data. Report the training and test errors for your classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LDA(x, mean, avg_cov):\n",
    "    prob = {}\n",
    "    prob['Iris-setosa'] = P(x,mean['setosa'],avg_cov)\n",
    "    prob['Iris-versicolor'] = P(x,mean['versicolor'],avg_cov)\n",
    "    prob['Iris-virginica'] = P(x,mean['virginica'],avg_cov)\n",
    "    \n",
    "    # key is set to prob.get since we need to compare the values of the dictionary, not the keys\n",
    "    return max(prob, key=prob.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate for LDA on training subset: 2.5%\n",
      "Error rate for LDA on testing subset: 0.0%\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy by comparing LDA/QDA prediction vs actual value\n",
    "def calculate_accuracy(classifier, subset):\n",
    "    n_correct = 0\n",
    "    for row in subset.iterrows():\n",
    "        x = np.array(row[1][0:4])\n",
    "        actual = row[1][4]\n",
    "        # classifier = 1: LDA, classifier = 2: QDA, classifier = 3: QDA with independent features\n",
    "        if classifier == 1:\n",
    "            if LDA(x, means, cov_avg) == actual:\n",
    "                n_correct += 1\n",
    "        elif classifier == 2:\n",
    "            if QDA(x, means, covs) == actual:\n",
    "                n_correct += 1\n",
    "        elif classifier == 3:\n",
    "            if QDA(x, means, indep_covs) == actual:\n",
    "                n_correct += 1\n",
    "        else:\n",
    "            raise ValueError(\"Classifier unknown. Please try again\")\n",
    "    accuracy = (n_correct/float(len(subset)) * 100)\n",
    "    error = 100 - accuracy\n",
    "    return str(error)\n",
    "print(\"Error rate for LDA on training subset: \" + calculate_accuracy(1, train) + \"%\")\n",
    "print(\"Error rate for LDA on testing subset: \" + calculate_accuracy(1, test) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Build a QDA classifier based on the training data. Report the training and test errors for your classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QDA(x, mean, covs):\n",
    "    prob = {}\n",
    "    prob['Iris-setosa'] = P(x,mean['setosa'],covs['setosa'])\n",
    "    prob['Iris-versicolor'] = P(x,mean['versicolor'],covs['versicolor'])\n",
    "    prob['Iris-virginica'] = P(x,mean['virginica'],covs['virginica'])\n",
    "    \n",
    "    # key is set to prob.get since we need to compare the values of the dictionary, not the keys\n",
    "    return max(prob, key=prob.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate for QDA on training subset: 1.6666666666666714%\n",
      "Error rate for QDA on testing subset: 0.0%\n"
     ]
    }
   ],
   "source": [
    "print(\"Error rate for QDA on training subset: \" + calculate_accuracy(2, train) + \"%\")\n",
    "print(\"Error rate for QDA on testing subset: \" + calculate_accuracy(2, test) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Is there any class linearly separable from other classes? Explain your answer based on your experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris-setosa error rate: 0.0 %\n",
      "Iris-versicolor error rate: 4.0 %\n",
      "Iris-virginica error rate: 2.0 %\n"
     ]
    }
   ],
   "source": [
    "# do training & testing sets together, rather than separated\n",
    "categories = ['Iris-setosa','Iris-versicolor','Iris-virginica']\n",
    "\n",
    "# run LDA on each separate class\n",
    "for category in categories:\n",
    "    flower_class = data[data[4] == category]\n",
    "    n_correct = 0\n",
    "    for row in flower_class.iterrows():\n",
    "        x = np.array(row[1][0:4])\n",
    "        actual = row[1][4]\n",
    "        predicted = LDA(x,means,cov_avg)\n",
    "        # if predicted answer matches our actual answer, we consider that a success\n",
    "        if predicted == actual:\n",
    "            n_correct += 1\n",
    "    accuracy = (n_correct/float(len(flower_class)) * 100)\n",
    "    error = 100 - accuracy\n",
    "    print(category, \"error rate:\", error, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these results, it's clear that 'Iris-setosa' is linearly separable from the other classes since it achieved perfect classification from LDA whereas the other classes did not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5:  Assume the features are independent, i.e., âˆ‘ is a diagonal matrix. Repeat Step 3, and report your results. Also, please report the training time of this method and the original QDA that you implemented in Step 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.12784375 0.         0.         0.        ]\n",
      " [0.         0.1294     0.         0.        ]\n",
      " [0.         0.         0.02884375 0.        ]\n",
      " [0.         0.         0.         0.00969375]]\n",
      "[[0.2669     0.         0.         0.        ]\n",
      " [0.         0.1081     0.         0.        ]\n",
      " [0.         0.         0.19844375 0.        ]\n",
      " [0.         0.         0.         0.042     ]]\n",
      "[[0.45624375 0.         0.         0.        ]\n",
      " [0.         0.1104     0.         0.        ]\n",
      " [0.         0.         0.33669375 0.        ]\n",
      " [0.         0.         0.         0.0724    ]]\n"
     ]
    }
   ],
   "source": [
    "#convert cov matrices to diagonal (set non-diag entries to 0)\n",
    "indep_covs = {}\n",
    "for category, cov in covs.items():\n",
    "    \n",
    "    # setup each category to have a 4 x 4 identity matrix\n",
    "    indep_covs[category] = np.zeros(cov.shape)\n",
    "    \n",
    "    # we should only add the diagonal values from our covariance matrices to our identity matrix\n",
    "    for row in range(cov.shape[0]):\n",
    "        for col in range(cov.shape[1]):\n",
    "            if row == col:\n",
    "                indep_covs[category][row][col] = cov[row][col]\n",
    "    print(indep_covs[category])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate for QDA on training subset: 1.6666666666666714%\n",
      "Error rate for QDA on testing subset: 0.0%\n",
      "Time taken for QDA with independent features: 61.92822265625 ms\n",
      "\n",
      "Error rate for QDA on training subset with independent features : 4.166666666666657%\n",
      "Error rate for QDA on testing subset with independent features : 0.0%\n",
      "Time taken for QDA with independent features: 56.9677734375 ms\n"
     ]
    }
   ],
   "source": [
    "# calculate the time and error rates for QDA (like in Step 3)\n",
    "qda_start_time = time.time() * 1000\n",
    "print(\"Error rate for QDA on training subset: \" + calculate_accuracy(2, train) + \"%\")\n",
    "print(\"Error rate for QDA on testing subset: \" + calculate_accuracy(2, test) + \"%\")\n",
    "print(\"Time taken for QDA with independent features:\", (time.time()*1000)-qda_start_time, \"ms\\n\")\n",
    "\n",
    "# calculate the time and error rates for QDA with independent features\n",
    "indep_start_time = time.time() * 1000\n",
    "print(\"Error rate for QDA on training subset with independent features : \" + calculate_accuracy(3, train) + \"%\")\n",
    "print(\"Error rate for QDA on testing subset with independent features : \" + calculate_accuracy(3, test) + \"%\")\n",
    "print(\"Time taken for QDA with independent features:\", (time.time()*1000)-indep_start_time, \"ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these results, it's clear that by assuming the features are independent, the computation time is much shorter compared to the original QDA function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
